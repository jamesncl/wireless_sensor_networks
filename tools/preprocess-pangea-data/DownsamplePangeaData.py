#!/usr/bin/env python

import os.path
import argparse
import sys
import numpy
from datetime import datetime
from datetime import timedelta
from PlotRadiationGraphs import plot_monthly


def parse_args():
    parser = argparse.ArgumentParser(description="Downsample Combined file generated by PreprocessPangeaData")
    parser.add_argument('-i', '--infolder', type=str, required=False, default='./',
                        help="Path to folder containing 1 level of subdirectories, each subdir a separate station. "
                             "Defaults to current directory")
    parser.add_argument('-g', '--granularities', type=int, required=False, nargs='*', dest='granularities',
                        default=[10, 30, 60, 180, 360, 720],
                        help='Granularities to output, in minutes, space separated. Defaults to 10 30 60 180 360 720 '
                             '(10 minutes, half hour, 1 hour, 3 hours, 6 hours, 12 hours)')
    return parser.parse_args()


def downsample_by_averaging(array, downsample_factor):
    """
        Down-sample an array by averaging consecutive blocks of size downsample_factor
        Used to convert minutely samples into 30 minute, 60 minute etc.

        If the size of array is not a multiple of downsample_factor, any remaining rows
        are discarded
    """
    end = downsample_factor * int(len(array)/downsample_factor)
    return numpy.mean(array[:end].reshape(-1, downsample_factor), 1)


if __name__ == '__main__':
    args = parse_args()

    if not os.path.isdir(args.infolder):
        print "Input folder does not exist"
        sys.exit()

    # Scan the infolder
    for root, sub_dirs, root_files in os.walk(args.infolder):

        # Data files should be in the following structure:
        # infolder
        #   |
        #   - Camborne
        #       |
        #       - Camborne-Combined-Minutely.csv
        #       - ... other files
        #   - Lerwick
        #       |
        #       - Lerwick-Combined-Minutely.csv
        #       - ... other files

        # Loop over every subdirectory found
        for sub_dir in sub_dirs:
            sub_dir_path = os.path.join(root, sub_dir)

            # Find the minutely file
            minutely_file_path = os.path.join(sub_dir_path, "{}-Combined-1-Minute.csv".format(sub_dir))

            if not os.path.isfile(minutely_file_path):
                print "Cannot find minutely file {}".format(minutely_file_path)
                sys.exit()

            # Create numpy array
            minutely_data = numpy.genfromtxt(minutely_file_path, delimiter=' ')

            # For every granularity specified
            for granularity in args.granularities:

                if granularity > 720:
                    sys.exit("Not sure how to handle down-sampling to a larger period than 12 hours - "
                             "e.g. for day, would you want to only average over the day time and"
                             "leave the night time at zero radiation?")

                downsampled_time_elapsed = None
                downsampled_radiation = None

                # Special handling for 720 minutes (12 hours)
                # It makes sense to average over 6am to 6pm, and 6pm to 6am, so we get a nice split
                # over night time / day time. Otherwise we don't reflect the drop at night time very well
                if granularity == 720:

                    # downsample the first 6 hours individually first:
                    downsampled_time_first_6_hours = downsample_by_averaging(minutely_data[0:360, 0], granularity)
                    downsampled_radiation_first_6_hours = downsample_by_averaging(minutely_data[0:360, 1], granularity)

                    # then downsample the rest
                    downsampled_time_after_6_hours = downsample_by_averaging(minutely_data[361:, 0], granularity)
                    downsampled_radiation_after_6_hours = downsample_by_averaging(minutely_data[361:, 1], granularity)

                    # then concatenate them
                    downsampled_time_elapsed = numpy.concatenate((downsampled_time_first_6_hours, downsampled_time_after_6_hours))
                    downsampled_radiation = numpy.concatenate((downsampled_radiation_first_6_hours, downsampled_radiation_after_6_hours))

                else:

                    # Downsample data according to the granularity
                    downsampled_time_elapsed = downsample_by_averaging(minutely_data[:, 0], granularity)
                    downsampled_radiation = downsample_by_averaging(minutely_data[:, 1], granularity)

                # Save as a new file
                output_file_path = os.path.join(sub_dir_path, "{}-Combined-{}-Minutes.csv".format(sub_dir, granularity))
                numpy.savetxt(output_file_path,
                              numpy.column_stack((downsampled_time_elapsed, downsampled_radiation)),
                              fmt='%.1f',
                              delimiter=' ')

                # Create plots

                # First, create the dictionary format the plotting function expects
                # graph_data = {
                #    '2007-01':
                #    {
                #        'x': [datetime(2007,1,1,0,0,0), datetime(2007,1,1,0,1,0), ...]
                #        'y': [0, 0, 0, 0, 0, 10, 34, 123...]
                #    }
                #    '2007-02:   ...
                #        ...
                # }
                graph_data = {}

                # At this stage the year has been stripped out of the data (we just have elapsed minutes, starting
                # at zero from midnight Jan 1st.
                # We need to convert this back into proper dates for plotting pretty graphs with date labels etc.

                # Convert elapsed time into date object, starting from Jan 1st midnight, using 2017 as an arbitrary year
                base_datetime = datetime(year=2017, month=1, day=1, minute=0, second=0)
                # Note that the data is time in *SECONDS* - even though the granularity of the original data is
                # minutely, the actual values are in SECONDS (i.e. 0, 60, 120, 180 ... etc)
                downsampled_datetime = [base_datetime + timedelta(seconds=s) for s in downsampled_time_elapsed]

                # Combine into a single frame so we can slice off months at a time
                downsampled_frame = numpy.column_stack((downsampled_datetime, downsampled_radiation))

                # Now we can create the graphing dictionary in the correct format
                for month_number in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]:

                    # select subset where month is month_number
                    # ms[0] refers to the datetime object in the numpy array of [datetime, radiation] values
                    month_subset = [ms.tolist() for ms in downsampled_frame if ms[0].month == month_number]

                    # Add the month subset to the graphing dictionary
                    graph_data[str(month_number)] = {

                        'x': [row[0] for row in month_subset],  # row[0] refers to datetime column
                        'y': [row[1] for row in month_subset]   # row[1] radiation column
                    }

                # Graph the data
                # sub_dir is the station name. sub_dir_path is used as the output folder.
                plot_monthly(graph_data, sub_dir, sub_dir_path, granularity)

                print "Downsampled {} by {}".format(sub_dir, granularity)